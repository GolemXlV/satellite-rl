{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=12, num_gpus=1, memory=1024 * 1024 * 1024 * 10, object_store_memory=1024 * 1024 * 1024 * 30, \n",
    "#          use_pickle=True,\n",
    "#          temp_dir='/home/projects/satellite_rl',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "import gym\n",
    "\n",
    "def choose_env_for(env_config):\n",
    "    print(env_config)\n",
    "    print(\"worker index is {}\".format(env_config.worker_index))\n",
    "    print(\"testing vector_index {}\".format(env_config.vector_index))\n",
    "    mod = env_config.worker_index\n",
    "    if env_config.worker_index > 0:\n",
    "        mod -= 1\n",
    "    sat_id = mod * 12 + env_config.vector_index\n",
    "#     env = gym.make(\"satellite_gym:SatelliteEnv-v1\", sat_id=sat_id)\n",
    "    env = gym.make(\"satellite_gym:SatelliteEnv-v2\", sat_id=sat_id)\n",
    "    return env\n",
    "\n",
    "register_env(\"SatelliteMultiEnv-v2\", lambda x: choose_env_for(x))\n",
    "# register_env(\"SatelliteMultiEnv-v1\", lambda x: choose_env_for(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ddpg.apex as apex\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "def on_train_result(info):\n",
    "    result = info[\"result\"]\n",
    "    phase = int(result[\"episode_reward_mean\"]) // int(result[\"episode_len_mean\"] - 1)\n",
    "#     if result[\"episode_reward_mean\"] > 40:\n",
    "#         phase = 2\n",
    "#     elif result[\"episode_reward_mean\"] > 20:\n",
    "#         phase = 1\n",
    "#     else:\n",
    "#         phase = 0\n",
    "#     phase = 2\n",
    "#     phase = 0\n",
    "    trainer = info[\"trainer\"]\n",
    "    trainer.workers.foreach_worker(\n",
    "        lambda ev: ev.foreach_env(\n",
    "            lambda env: env.set_phase(phase)))\n",
    "\n",
    "config = apex.APEX_DDPG_DEFAULT_CONFIG.copy()\n",
    "config['model']['use_lstm'] = True\n",
    "config[\"model\"][\"vf_share_layers\"] = True\n",
    "\n",
    "# config[\"model\"][\"fcnet_hiddens\"] = [512, 512]\n",
    "# config[\"model\"][\"lstm_cell_size\"] = 512\n",
    "# config[\"actor_hiddens\"] = [500, 400, 300]\n",
    "# config[\"critic_hiddens\"] = [500, 400, 300]\n",
    "\n",
    "# config[\"optimizer\"][\"batch_replay\"] = True\n",
    "# config[\"num_workers\"] = 50\n",
    "config[\"num_workers\"] = 10\n",
    "config[\"num_cpus_per_worker\"] = .1\n",
    "config[\"seed\"] = 0\n",
    "config[\"eager\"] = False\n",
    "\n",
    "\n",
    "# Discount factor to 0\n",
    "# config[\"gamma\"] = 0\n",
    "\n",
    "# 1-Step Q-learning\n",
    "# config[\"n_step\"] = 5\n",
    "\n",
    "# Disable exploration\n",
    "# config[\"per_worker_exploration\"] = False\n",
    "# config[\"exploration_should_anneal\"] = True\n",
    "# config[\"schedule_max_timesteps\"] = 10000000\n",
    "# config[\"exploration_ou_noise_scale\"] = 1e-4\n",
    "# config[\"exploration_fraction\"] = 0\n",
    "# config[\"exploration_final_scale\"] = 0\n",
    "# config[\"exploration_final_eps\"] = 0\n",
    "# config[\"pure_exploration_steps\"] = 0\n",
    "\n",
    "config[\"clip_rewards\"] = False\n",
    "# config[\"use_huber\"] = True\n",
    "# config[\"actor_lr\"] = config[\"critic_lr\"] = config[\"lr\"] = 5e-3\n",
    "# config[\"tau\"] = 1.0 # value_network + 1-tau/tau * target_network\n",
    "# config[\"evaluation_interval\"] = 5\n",
    "# config[\"evaluation_num_episodes\"] = 10\n",
    "# config[\"target_network_update_freq\"] = 50000\n",
    "# config[\"buffer_size\"] = 4000000\n",
    "# config[\"observation_filter\"] = \"NoFilter\"\n",
    "# config[\"train_batch_size\"] = 2000\n",
    "# config[\"sample_batch_size\"] = 200\n",
    "config[\"callbacks\"] = { \"on_train_result\": on_train_result }\n",
    "config[\"num_envs_per_worker\"] = 12\n",
    "config[\"env_config\"][\"num_envs_per_worker\"] = config[\"num_envs_per_worker\"]\n",
    "\n",
    "# trainer = apex.ApexDDPGTrainer(config=config, env=\"satellite_gym:SatelliteEnv-v1\")\n",
    "# trainer = apex.ApexDDPGTrainer.with_updates(default_config=config)\n",
    "trainer = apex.ApexDDPGTrainer(config=config, env=\"SatelliteMultiEnv-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler as ASHA\n",
    "\n",
    "\n",
    "scheduler = ASHA(metric=\"episode_reward_mean\", mode=\"max\", max_t=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "   trainer,\n",
    "   stop={\"training_iteration\": 200, \"episode_reward_mean\": 23},\n",
    "   config={\n",
    "      \"env\": \"SatelliteMultiEnv-v2\",\n",
    "      \"actor_lr\": tune.grid_search([1e-5, 3e-3, 3e-4, 3e-5]),\n",
    "      \"critic_lr\": tune.grid_search([1e-5, 3e-3, 3e-4, 3e-5]),\n",
    "      \"tau\": tune.grid_search([0., 2e-2, 1e-3, 1]),\n",
    "      \"gamma\": tune.grid_search([0, .9, .95, .99]),\n",
    "      \"n_step\": tune.grid_search([1, 3, 5]),\n",
    "   },\n",
    "   scheduler=scheduler,\n",
    "   checkpoint_freq=50,\n",
    "   checkpoint_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.restore('/home/golemxiv/ray_results/APEX_DDPG/APEX_DDPG_SatelliteMultiEnv-v2_5a550b8a_2020-02-06_12-47-4846v4tr6x/checkpoint_200/checkpoint-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config['critic_lr'] = 1e-5\n",
    "trainer.config['actor_lr'] = 1e-5\n",
    "trainer.config[\"exploration_should_anneal\"] = True\n",
    "trainer.config[\"exploration_final_scale\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_policy().export_model('trained_model_v7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# env = gym.make(\"satellite_gym:SatelliteEnv-v1\", sat_id=8)\n",
    "# env = gym.make(\"satellite_gym:SatelliteEnv-v2\", sat_id=18)\n",
    "env = gym.make(\"satellite_gym:SatelliteEnv-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satellite_gym.satellite_env import TRAIN_COLUMNS, TEST_COLUMNS\n",
    "import numpy as np\n",
    "\n",
    "trainer.get_policy().compute_single_action(obs=np.squeeze(env.train_data[0]), state=trainer.get_policy().get_initial_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()\n",
    "env.set_phase(1)\n",
    "true_value = env.test_data[:env.mod * (env.phase + 1),:3]\n",
    "\n",
    "predicted_value = policy.compute_actions(obs_batch=env.current_data, state=policy.get_initial_state())[0][:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig) #<-- Note the difference from your original code...\n",
    "ax.scatter(xs=true_value[:,:1], ys=true_value[:,1:2], zs=true_value[:,2:3], marker='o')\n",
    "ax.scatter(xs=predicted_value[:,:1], ys=predicted_value[:,1:2], zs=predicted_value[:,2:3], marker='^')\n",
    "ax.view_init(elev=10., azim=40)\n",
    "# for ii in range(0,360,1):\n",
    "#         ax.view_init(elev=10., azim=ii)\n",
    "#         fig.savefig(\"movie/movie%d.png\" % ii)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"satellite_gym:SatelliteEnvTest-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.get_policy()\n",
    "true_value = env.current_data[:,:3]\n",
    "\n",
    "predicted_value = policy.compute_actions(obs_batch=env.current_data, state=policy.get_initial_state())[0][:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig) #<-- Note the difference from your original code...\n",
    "ax.scatter(xs=true_value[:,:1], ys=true_value[:,1:2], zs=true_value[:,2:3], marker='o')\n",
    "ax.scatter(xs=predicted_value[:,:1], ys=predicted_value[:,1:2], zs=predicted_value[:,2:3], marker='^')\n",
    "ax.view_init(elev=10., azim=180)\n",
    "# for ii in range(0,360,1):\n",
    "#         ax.view_init(elev=10., azim=ii)\n",
    "#         fig.savefig(\"movie/movie%d.png\" % ii)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idao]",
   "language": "python",
   "name": "conda-env-idao-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
