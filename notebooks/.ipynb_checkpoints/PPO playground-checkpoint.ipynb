{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=12, num_gpus=1, memory=1024 * 1024 * 1024 * 10, object_store_memory=1024 * 1024 * 1024 * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "import gym\n",
    "\n",
    "def choose_env_for(env_config):\n",
    "    print(env_config)\n",
    "    print(\"worker index is {}\".format(env_config.worker_index))\n",
    "    print(\"testing vector_index {}\".format(env_config.vector_index))\n",
    "    mod = env_config.worker_index\n",
    "    if env_config.worker_index > 0:\n",
    "        mod -= 1\n",
    "    sat_id = mod * env_config[\"num_envs_per_worker\"] + env_config.vector_index\n",
    "    env = gym.make(\"satellite_gym:SatelliteEnv-v2\", sat_id=sat_id)\n",
    "    return env\n",
    "\n",
    "register_env(\"SatelliteMultiEnv-v2\", lambda x: choose_env_for(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "def on_train_result(info):\n",
    "    result = info[\"result\"]\n",
    "    if result[\"episode_reward_mean\"] > 42:\n",
    "        phase = 2\n",
    "    elif result[\"episode_reward_mean\"] > 21:\n",
    "        phase = 1\n",
    "    else:\n",
    "        phase = 0\n",
    "    trainer = info[\"trainer\"]\n",
    "    trainer.workers.foreach_worker(\n",
    "        lambda ev: ev.foreach_env(\n",
    "            lambda env: env.set_phase(phase)))\n",
    "    \n",
    "    \n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['use_lstm'] = True\n",
    "config[\"model\"][\"vf_share_layers\"] = True\n",
    "# config[\"optimizer\"][\"batch_replay\"] = True\n",
    "config[\"num_workers\"] = 10\n",
    "config[\"num_gpus_per_worker\"] = .1\n",
    "config[\"seed\"] = 0\n",
    "config[\"eager\"] = False\n",
    "\n",
    "# config[\"clip_rewards\"] = False\n",
    "# config[\"tau\"] = 1.0 # 1-tau * value_network + 1-tau/tau * target_network\n",
    "# config[\"evaluation_interval\"] = 5\n",
    "# config[\"evaluation_num_episodes\"] = 10\n",
    "# config[\"exploration_ou_noise_scale\"] = 1.0\n",
    "# config[\"buffer_size\"] = 4000000\n",
    "# config[\"observation_filter\"] = \"NoFilter\"\n",
    "# config[\"train_batch_size\"] = 1024\n",
    "# config[\"sample_batch_size\"] = 100\n",
    "# config[\"num_envs_per_worker\"] = 30\n",
    "config[\"callbacks\"] = { \"on_train_result\": on_train_result }\n",
    "config[\"num_envs_per_worker\"] = 12\n",
    "config[\"env_config\"][\"num_envs_per_worker\"] = config[\"num_envs_per_worker\"]\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config, env=\"SatelliteMultiEnv-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(201):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idao]",
   "language": "python",
   "name": "conda-env-idao-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
