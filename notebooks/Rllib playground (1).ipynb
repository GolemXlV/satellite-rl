{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satellite_gym.envs.satellite_env import SatelliteEnv\n",
    "# from satellite_gym.envs.satellite_env import SatelliteEnvV0 as SatelliteEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from satellite_gym.envs.satellite_env.satellite_env import TRAIN_COLUMNS, TEST_COLUMNS\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "COLUMNS = ['id', 'sat_id', 'x', 'y', 'z', 'Vx', 'Vy', 'Vz', 'x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim']\n",
    "SATELLITES_NUM = 300\n",
    "\n",
    "df = pd.read_csv(Path('./data/train.csv'), index_col='id', usecols=COLUMNS, dtype=np.float64)\n",
    "\n",
    "df[\"orbit_sim\"] = df.apply(lambda x: np.sqrt(x['x_sim']**2 + x['y_sim']**2 + x['z_sim']**2), axis=1)\n",
    "df[\"V_sim\"] = df.apply(lambda x: np.sqrt(x['Vx_sim']**2 + x['Vy_sim']**2 + x['Vz_sim']**2), axis=1)\n",
    "\n",
    "columns = TEST_COLUMNS + TRAIN_COLUMNS\n",
    "\n",
    "scaler = MinMaxScaler((-10,10))\n",
    "df[columns] = scaler.fit_transform(df[columns])\n",
    "\n",
    "# sat_id = randint(0, SATELLITES_NUM)\n",
    "sat_id = 41\n",
    "# df = df[df['sat_id'] == sat_id]  # take random satellite\n",
    "# df.drop('sat_id', inplace=True, axis=1)\n",
    "\n",
    "# df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \n",
    "from satellite_gym.envs.satellite_env.satellite_env import TRAIN_COLUMNS, TEST_COLUMNS\n",
    "\n",
    "env = SatelliteEnv(df)\n",
    "COLS = ['x_sim', 'y_sim', 'z_sim']\n",
    "\n",
    "arr1=env.df[TRAIN_COLUMNS].values\n",
    "arr2=env.df[TEST_COLUMNS].values\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig) #<-- Note the difference from your original code...\n",
    "ax.scatter(xs=arr1[:,:1], ys=arr1[:,1:2], zs=arr1[:,2:3], marker='o')\n",
    "ax.scatter(xs=arr2[:,:1], ys=arr2[:,1:2], zs=arr2[:,2:3], marker='^')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=12, num_gpus=1, memory=1024 * 1024 * 1024 * 20, object_store_memory=1024 * 1024 * 1024 * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env = SatelliteEnv(df, sat_id=sat_id)\n",
    "register_env(\"SatelliteEnv-v2\", lambda x: env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.a3c as a3c\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "config = a3c.DEFAULT_CONFIG.copy()\n",
    "config['model']['use_lstm'] = True\n",
    "config[\"num_gpus\"] = 1\n",
    "config[\"num_workers\"] = 9\n",
    "config[\"eager\"] = False\n",
    "trainer = a3c.A3CTrainer(config=config, env=\"SatelliteEnv-v2\")\n",
    "\n",
    "for i in range(200):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "def on_train_result(info):\n",
    "    result = info[\"result\"]\n",
    "    if result[\"episode_reward_mean\"] > 44:\n",
    "        phase = 2\n",
    "    elif result[\"episode_reward_mean\"] > 22:\n",
    "        phase = 1\n",
    "    else:\n",
    "        phase = 0\n",
    "    trainer = info[\"trainer\"]\n",
    "    trainer.workers.foreach_worker(\n",
    "        lambda ev: ev.foreach_env(\n",
    "            lambda env: env.set_phase(phase)))\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['use_lstm'] = True\n",
    "config[\"model\"][\"vf_share_layers\"] = True\n",
    "# config[\"lr\"] = 0.001\n",
    "# config[\"gamma\"] = 0.9\n",
    "# config[\"lambda\"] = 0.9\n",
    "# config[\"num_gpus\"] = 1\n",
    "config[\"num_workers\"] = 5\n",
    "config[\"num_cpus_per_worker\"] = 2\n",
    "config[\"num_gpus_per_worker\"] = .2\n",
    "config[\"seed\"] = 0\n",
    "config[\"eager\"] = False\n",
    "# config[\"vf_clip_param\"] = 1000.0\n",
    "# config[\"vf_loss_coeff\"] = 0.\n",
    "# config[\"sample_batch_size\"] = 200\n",
    "# config[\"train_batch_size\"] = 1000\n",
    "# config[\"batch_mode\"] = \"complete_episodes\"\n",
    "# config[\"shuffle_sequences\"] = False\n",
    "# config[\"entropy_coeff\"] = .5\n",
    "# config[\"kl_coeff\"] = .5\n",
    "config[\"callbacks\"] = { \"on_train_result\": on_train_result }\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config, env=\"SatelliteEnv-v2\")\n",
    "\n",
    "\n",
    "for i in range(401):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import numpy as np\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['model']['use_lstm'] = True\n",
    "config[\"model\"][\"vf_share_layers\"] = True\n",
    "\n",
    "async_hb_scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    max_t=200,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=3\n",
    ")\n",
    "\n",
    "\n",
    "def train(config, reporter):\n",
    "    trainer = ppo.PPOTrainer(config=config, env=\"SatelliteEnv-v2\")\n",
    "    while True:\n",
    "        result = trainer.train()\n",
    "        reporter(**result)\n",
    "        if result[\"episode_reward_mean\"] > 44:\n",
    "            phase = 2\n",
    "        elif result[\"episode_reward_mean\"] > 22:\n",
    "            phase = 1\n",
    "        else:\n",
    "            phase = 0\n",
    "        trainer.workers.foreach_worker(\n",
    "            lambda ev: ev.foreach_env(\n",
    "                lambda env: env.set_phase(phase)))\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    train,\n",
    "    stop={\"training_iteration\": 200},\n",
    "    config={\n",
    "        \"env\": \"SatelliteEnv-v2\",\n",
    "#         \"num_gpus\": .2,\n",
    "        \"num_workers\": 2,\n",
    "#         \"num_gpus_per_worker\": .05,\n",
    "#         \"num_cpus_per_worker\": .2,\n",
    "#         \"vf_clip_param\": 1000.0,\n",
    "#         \"sample_batch_size\": tune.grid_search([200, 400, 1000]),\n",
    "#         \"train_batch_size\": tune.grid_search([1000, 2000]),\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001, 0.00005]),\n",
    "        \"gamma\": tune.grid_search(list(np.linspace(0.9, 0.99, 3))),\n",
    "#         \"lambda\": tune.grid_search(list(np.linspace(0.9, 0.99, 3))),\n",
    "#         \"vf_loss_coeff\": tune.grid_search(list(np.linspace(0, 1, 5))),\n",
    "#         \"kl_coeff\": tune.grid_search([0.1, .2, .5]),\n",
    "#         \"entropy_coeff\": tune.grid_search([0., .5, 1]),\n",
    "        \"eager\": False,\n",
    "        \"seed\": 0,\n",
    "        \"batch_mode\": tune.grid_search([\"truncate_episodes\", \"complete_episodes\"]),\n",
    "        \"shuffle_sequences\": tune.grid_search([False, True]),\n",
    "        \"model\": config[\"model\"]\n",
    "    }, scheduler=async_hb_scheduler,\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": .2,\n",
    "        \"extra_cpu\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import Analysis\n",
    "import pandas as pd\n",
    "analysis = Analysis(\"~/ray_results/PPO\")\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "df.sort_values([\"episode_reward_mean\"], ascending=False).iloc[0].loc[\"experiment_tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"episode_reward_mean\"], ascending=False).iloc[1].loc[\"experiment_tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"episode_reward_mean\"], ascending=False).iloc[2].loc[\"experiment_tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"episode_reward_mean\"], ascending=False).iloc[3].loc[\"experiment_tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('trained_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "trainer.restore('/home/golemxiv/ray_results/PPO_SatelliteEnv-v2_2020-01-23_12-33-45fmpi6yi3/checkpoint_2001/checkpoint_2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_policy().export_model('trained_model_v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# obs = np.zeros((256), dtype=np.float32)\n",
    "obs = np.squeeze(env.df.head(256).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(df.head(1).values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(env.df.head(1).values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satellite_gym.envs.satellite_env.satellite_env import TRAIN_COLUMNS, TEST_COLUMNS\n",
    "trainer.get_policy().compute_single_action(obs=np.squeeze(env.df[TRAIN_COLUMNS].head(1).values), state=trainer.get_policy().get_initial_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_value = env.df[['Vx', 'Vy', 'Vz']].values\n",
    "\n",
    "policy = trainer.get_policy()\n",
    "state = policy.get_initial_state()\n",
    "predicted_value = np.empty((1, 3,))\n",
    "for i in range(len(true_value)):\n",
    "#     if len(predicted_value) == 1:\n",
    "#         predicted_value = np.array([env.df[['Vx_sim', 'Vy_sim', 'Vz_sim']].loc[i].values])\n",
    "    val = policy.compute_single_action(np.squeeze(env.df[TRAIN_COLUMNS].loc[i].values), state=state)\n",
    "    state = val[1]\n",
    "    predicted_value = np.append(predicted_value, [val[0][3:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig) #<-- Note the difference from your original code...\n",
    "ax.scatter(xs=true_value[:,:1], ys=true_value[:,1:2], zs=true_value[:,2:3], marker='o')\n",
    "ax.scatter(xs=predicted_value[:,:1], ys=predicted_value[:,1:2], zs=predicted_value[:,2:3], marker='^')\n",
    "ax.view_init(elev=10., azim=20)\n",
    "plt.show()\n",
    "# for ii in range(0,360,1):\n",
    "#         ax.view_init(elev=10., azim=ii)\n",
    "#         fig.savefig(\"movie/movie%d.png\" % ii)\n",
    "\n",
    "\n",
    "#ax + by + cz + d = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(x=true_value[:,:1], y=true_value[:,1:2], z=true_value[:,2:3], mode='markers')])\n",
    "fig.write_image(\"figure.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idao]",
   "language": "python",
   "name": "conda-env-idao-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
