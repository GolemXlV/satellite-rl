{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=12, num_gpus=1, memory=1024 * 1024 * 1024 * 10, object_store_memory=1024 * 1024 * 1024 * 30, \n",
    "#          use_pickle=True,\n",
    "#          temp_dir='/home/projects/satellite_rl',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "import gym\n",
    "\n",
    "def env_creator(env_config):\n",
    "    import gym, satellite_gym\n",
    "    return gym.make(\"satellite_gym:SatelliteEnv-v1\", sat_id=41)\n",
    "\n",
    "# env = gym.make(\"satellite_gym:SatelliteEnv-v1\", sat_id=41)\n",
    "# env_creator = lambda x: env\n",
    "# env = SatelliteEnv(df, sat_id=41)\n",
    "register_env(\"SatelliteEnv-v2\", lambda x: env_creator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ars as ars\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "def on_train_result(info):\n",
    "    result = info[\"result\"]\n",
    "    if result[\"episode_reward_mean\"] > 45:\n",
    "        phase = 2\n",
    "    elif result[\"episode_reward_mean\"] > 22:\n",
    "        phase = 1\n",
    "    else:\n",
    "        phase = 0\n",
    "    trainer = info[\"trainer\"]\n",
    "    trainer.workers.foreach_worker(\n",
    "        lambda ev: ev.foreach_env(\n",
    "            lambda env: env.set_phase(phase)))\n",
    "    \n",
    "    \n",
    "config = ars.DEFAULT_CONFIG.copy()\n",
    "# config['model']['use_lstm'] = True\n",
    "# config[\"model\"][\"vf_share_layers\"] = True\n",
    "config[\"num_workers\"] = 10\n",
    "config[\"seed\"] = 0\n",
    "config[\"eager\"] = False\n",
    "config[\"noise_stdev\"] = .01\n",
    "config[\"num_rollouts\"] = 1\n",
    "config[\"rollouts_used\"] = 1\n",
    "config[\"sgd_stepsize\"] = .02\n",
    "config[\"noise_size\"] = 2500000000\n",
    "config[\"eval_prob\"] = .02\n",
    "config[\"observation_filter\"] = \"NoFilter\"\n",
    "# config[\"callbacks\"] = { \"on_train_result\": on_train_result }\n",
    "\n",
    "trainer = ars.ARSTrainer(config=config, env=\"satellite_gym:SatelliteEnv-v1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(4010):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray.rllib.agents.ars as ars\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import numpy as np\n",
    "\n",
    "config = ars.DEFAULT_CONFIG.copy()\n",
    "config[\"observation_filter\"] = \"NoFilter\"\n",
    "# config['model']['use_lstm'] = True\n",
    "# config[\"model\"][\"vf_share_layers\"] = True\n",
    "\n",
    "async_hb_scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    max_t=200,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=3\n",
    ")\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"noise_stdev\": (0.01, 0.1),\n",
    "#     \"num_rollouts\": (1, 32),\n",
    "#     \"rollouts_used\": (1, 32),\n",
    "    \"sgd_stepsize\": (0.01, 0.1),\n",
    "#     \"noise_size\": (2500000, 250000000),\n",
    "    \"eval_prob\": (0.01, 0.1),\n",
    "}\n",
    "\n",
    "bayes_search = BayesOptSearch(space, max_concurrent=4, metric=\"episode_reward_mean\", mode=\"max\", \n",
    "                              utility_kwargs={\n",
    "            \"kind\": \"ucb\",\n",
    "            \"kappa\": 2.5,\n",
    "            \"xi\": 0.0\n",
    "        }, use_early_stopped_trials=True)\n",
    "\n",
    "# def train(config, reporter):\n",
    "#     trainer = ppo.PPOTrainer(config=config, env=\"SatelliteEnv-v2\")\n",
    "#     while True:\n",
    "#         result = trainer.train()\n",
    "#         reporter(**result)\n",
    "#         if result[\"episode_reward_mean\"] > 44:\n",
    "#             phase = 2\n",
    "#         elif result[\"episode_reward_mean\"] > 22:\n",
    "#             phase = 1\n",
    "#         else:\n",
    "#             phase = 0\n",
    "#         trainer.workers.foreach_worker(\n",
    "#             lambda ev: ev.foreach_env(\n",
    "#                 lambda env: env.set_phase(phase)))\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    \"ARS\",\n",
    "    stop={\"training_iteration\": 200},\n",
    "    config={\n",
    "        \"env\": \"satellite_gym:SatelliteEnv-v1\",\n",
    "        \"num_workers\": 10,\n",
    "        \"eager\": False,\n",
    "        \"seed\": 0,\n",
    "        \"observation_filter\": \"NoFilter\",\n",
    "    }, \n",
    "    scheduler=async_hb_scheduler, \n",
    "    search_alg=bayes_search,\n",
    "#     resources_per_trial={\n",
    "#         \"cpu\": 1,\n",
    "#         \"gpu\": .2,\n",
    "#         \"extra_cpu\": 1,\n",
    "#     },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray.rllib.agents.ars as ars\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "\n",
    "config = ars.DEFAULT_CONFIG.copy()\n",
    "# config['model']['use_lstm'] = True\n",
    "# config[\"model\"][\"vf_share_layers\"] = True\n",
    "\n",
    "async_hb_scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    max_t=200,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=3\n",
    ")\n",
    "\n",
    "\n",
    "def train(config, reporter):\n",
    "    trainer = ars.ARSTrainer(config=config, env=\"SatelliteEnv-v2\")\n",
    "    while True:\n",
    "        result = trainer.train()\n",
    "        reporter(**result)\n",
    "#         if result[\"episode_reward_mean\"] > 44:\n",
    "#             phase = 2\n",
    "#         elif result[\"episode_reward_mean\"] > 22:\n",
    "#             phase = 1\n",
    "#         else:\n",
    "#             phase = 0\n",
    "#         trainer.workers.foreach_worker(\n",
    "#             lambda ev: ev.foreach_env(\n",
    "#                 lambda env: env.set_phase(phase)))\n",
    "\n",
    "space = {\n",
    "    \"noise_stdev\": hp.loguniform(\"noise_stdev\", 1e-2, 0.1),\n",
    "#     \"num_rollouts\": hp.qlognormal('num_rollouts', 32, 1, 8),\n",
    "#     \"rollouts_used\":hp.qlognormal('rollouts_used', 32, 1, 8),  # number of perturbs to keep in gradient estimate\n",
    "    \"sgd_stepsize\": hp.loguniform(\"sgd_stepsize\", 1e-2, 0.1),  # sgd step-size\n",
    "#     \"noise_size\":hp.choice(\"noise_size\", [25000000, 250000000, 2500000000]),  # number of perturbs to keep in gradient estimate\n",
    "    \"eval_prob\": hp.loguniform(\"eval_prob\", 1e-2, 0.1),  # sgd step-size\n",
    "}\n",
    "\n",
    "hyperopt_search = HyperOptSearch(space, max_concurrent=4, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    \"ARS\",\n",
    "    stop={\"training_iteration\": 200},\n",
    "    config={\n",
    "        \"env\": \"SatelliteEnv-v2\",\n",
    "#         \"num_workers\": 1,\n",
    "        \"eager\": False,\n",
    "        \"seed\": 0,\n",
    "#         \"iterations\": 200,\n",
    "#         \"noise_stdev\": tune.uniform(0.01, 0.1),\n",
    "    }, \n",
    "    num_samples=10,\n",
    "    scheduler=async_hb_scheduler,\n",
    "    search_alg=hyperopt_search,\n",
    "#     resources_per_trial={\n",
    "#         \"cpu\": 1,\n",
    "# #         \"gpu\": .2,\n",
    "# #         \"extra_cpu\": 1,\n",
    "#     },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:idao]",
   "language": "python",
   "name": "conda-env-idao-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
